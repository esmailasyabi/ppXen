/****************************************************************************
 * (C) 2005-2006 - Emmanuel Ackaouy - XenSource Inc.
 ****************************************************************************
 *
 *        File: common/csched_credit.c
 *      Author: Emmanuel Ackaouy
 *
 * Description: Credit-based SMP CPU scheduler
 */

#include <xen/config.h>
#include <xen/init.h>
#include <xen/lib.h>
#include <xen/sched.h>
#include <xen/domain.h>
#include <xen/delay.h>
#include <xen/event.h>
#include <xen/time.h>
#include <xen/sched-if.h>
#include <xen/softirq.h>
#include <asm/atomic.h>
#include <asm/div64.h>
#include <xen/errno.h>
#include <xen/keyhandler.h>
#include <xen/trace.h>


/*
 * Basic constants
 */
#define CSCHED_DEFAULT_WEIGHT       256
//#define CSCHED_TICKS_PER_TSLICE     3
/* Default timeslice: 30ms */
//#define CSCHED_DEFAULT_TSLICE_MS    30
// showan
#define CSCHED_TICKS_PER_TSLICE     2 // showan?
#define CSCHED_DEFAULT_TSLICE_MS    10
#define CSCHED_CREDITS_PER_MSEC     10


/*
 * Priorities
 */
#define CSCHED_PRI_TS_BOOST      0      /* time-share waking up */
#define CSCHED_PRI_TS_UNDER     -1      /* time-share w/ credits */
#define CSCHED_PRI_TS_OVER      -2      /* time-share w/o credits */
#define CSCHED_PRI_IDLE         -64     /* idle */


/*
 * Flags
 */
#define CSCHED_FLAG_VCPU_PARKED    0x0  /* VCPU over capped credits */
#define CSCHED_FLAG_VCPU_YIELD     0x1  /* VCPU yielding */



// eaxen constants

#define EAXEN_BUDGETING_PERIOD_MS   100
#define EAXEN_DEFAULT_TSLICE_MS    10
#define EAXEN_DEFAULT_CORE_CAPACITY    100
#define SHOWAN_MAX_TOLERATED_IO_COLLISION 5
#define SHOWAN_BUDGET_CONSTANT  1000000   // nano








#define EAXEN_DEFAULT_BUDGET      20
#define SHOWAN_DEFAULT_IOQ      100 // all VMs are initially treated a CPU-bound VMs


/*
 * Useful macros
 */
#define CSCHED_PRIV(_ops)   \
    ((struct csched_private *)((_ops)->sched_data))
#define CSCHED_PCPU(_c)     \
    ((struct csched_pcpu *)per_cpu(schedule_data, _c).sched_priv)
#define CSCHED_VCPU(_vcpu)  ((struct csched_vcpu *) (_vcpu)->sched_priv)
#define CSCHED_DOM(_dom)    ((struct csched_dom *) (_dom)->sched_priv)
#define RUNQ(_cpu)          (&(CSCHED_PCPU(_cpu)->runq))
/* Is the first element of _cpu's runq its idle vcpu? */
#define IS_RUNQ_IDLE(_cpu)  (list_empty(RUNQ(_cpu)) || \
                             is_idle_vcpu(__runq_elem(RUNQ(_cpu)->next)->vcpu))


/*
 * CSCHED_STATS
 *
 * Manage very basic per-vCPU counters and stats.
 *
 * Useful for debugging live systems. The stats are displayed
 * with runq dumps ('r' on the Xen console).
 */
#ifdef SCHED_STATS

#define CSCHED_STATS

#define SCHED_VCPU_STATS_RESET(_V)                      \
    do                                                  \
    {                                                   \
        memset(&(_V)->stats, 0, sizeof((_V)->stats));   \
    } while ( 0 )

#define SCHED_VCPU_STAT_CRANK(_V, _X)       (((_V)->stats._X)++)

#define SCHED_VCPU_STAT_SET(_V, _X, _Y)     (((_V)->stats._X) = (_Y))

#else /* !SCHED_STATS */

#undef CSCHED_STATS

#define SCHED_VCPU_STATS_RESET(_V)         do {} while ( 0 )
#define SCHED_VCPU_STAT_CRANK(_V, _X)      do {} while ( 0 )
#define SCHED_VCPU_STAT_SET(_V, _X, _Y)    do {} while ( 0 )

#endif /* SCHED_STATS */


/*
 * Credit tracing events ("only" 512 available!). Check
 * include/public/trace.h for more details.
 */
#define TRC_CSCHED_SCHED_TASKLET TRC_SCHED_CLASS_EVT(CSCHED, 1)
#define TRC_CSCHED_ACCOUNT_START TRC_SCHED_CLASS_EVT(CSCHED, 2)
#define TRC_CSCHED_ACCOUNT_STOP  TRC_SCHED_CLASS_EVT(CSCHED, 3)
#define TRC_CSCHED_STOLEN_VCPU   TRC_SCHED_CLASS_EVT(CSCHED, 4)
#define TRC_CSCHED_PICKED_CPU    TRC_SCHED_CLASS_EVT(CSCHED, 5)
#define TRC_CSCHED_TICKLE        TRC_SCHED_CLASS_EVT(CSCHED, 6)


/*
 * Node Balancing
 */
#define CSCHED_BALANCE_NODE_AFFINITY    0
#define CSCHED_BALANCE_CPU_AFFINITY     1

/*
 * Boot parameters
 */
static int __read_mostly sched_credit_tslice_ms = CSCHED_DEFAULT_TSLICE_MS;
integer_param("sched_credit_tslice_ms", sched_credit_tslice_ms);

/*
 * Physical CPU
 */
struct csched_pcpu {
    struct list_head runq;
    uint32_t runq_sort_last;
    struct timer ticker;
    unsigned int tick;
    unsigned int idle_bias;
    /* Store this here to avoid having too many cpumask_var_t-s on stack */
    cpumask_var_t balance_mask;

    // eaxen fields

        int eaxen_core_capacity;

        uint32_t eaxen_core_budgeting_last;
        struct timer eaxen_ticker;
        int eaxen_core_max_filled_current_period;
        int eaxen_core_max_filled_last_period;
        // showan
        unsigned long showan_io_occupied_pri_cur_sr;  // showan? should be intialized?
        unsigned long showan_io_occupied_pri_last_sr;

        unsigned long showan_max_io_collision_last_sr;
        struct csched_vcpu *showan_svc_vcpu_with_max_io_col;
        unsigned int showan_pcpu_has_io_overflow;
        unsigned int showan_pcpu_has_cpu_overflow;
        unsigned int showan_cpu_load;



};

/*
 * Convenience macro for accessing the per-PCPU cpumask we need for
 * implementing the two steps (vcpu and node affinity) balancing logic.
 * It is stored in csched_pcpu so that serialization is not an issue,
 * as there is a csched_pcpu for each PCPU and we always hold the
 * runqueue spin-lock when using this.
 */
#define csched_balance_mask (CSCHED_PCPU(smp_processor_id())->balance_mask)

/*
 * Virtual CPU
 */
struct csched_vcpu {
    struct list_head runq_elem;
    struct list_head active_vcpu_elem;
    struct csched_dom *sdom;
    struct vcpu *vcpu;
    atomic_t credit;
    unsigned int residual;
    s_time_t start_time;   /* When we were scheduled (used for credit) */
    unsigned flags;
    int16_t pri;
    //eaxen
   // int eaxen_budget;
    long eaxen_asset;
    uint32_t eaxen_vcpu_budgeting_last;

    // showan

    int16_t   showan_i_o_pri;
    int16_t   showan_i_o_pri_curr;
    uint32_t  showan_i_o_collision;
    int16_t   showan_it_should_be_migrated_io_col;


#ifdef CSCHED_STATS
    struct {
        int credit_last;
        uint32_t credit_incr;
        uint32_t state_active;
        uint32_t state_idle;
        uint32_t migrate_q;
        uint32_t migrate_r;
        uint32_t kicked_away;
    } stats;
#endif
};

/*
 * Domain
 */
struct csched_dom {
    struct list_head active_vcpu;
    struct list_head active_sdom_elem;
    struct domain *dom;
    /* cpumask translated from the domain's node-affinity.
     * Basically, the CPUs we prefer to be scheduled on. */
    cpumask_var_t node_affinity_cpumask;
    uint16_t active_vcpu_count;
   // uint16_t weight;
    //uint16_t cap;
    unsigned int eaxen_dom_budget;  // eaxen
    unsigned int showan_dom_ioq;  // eaxen

};

/*
 * System-wide private data
 */
struct csched_private {
    /* lock for the whole pluggable scheduler, nests inside cpupool_lock */
    spinlock_t lock;
    struct list_head active_sdom;
    uint32_t ncpus;
    struct timer  master_ticker;
    unsigned int master;
    cpumask_var_t idlers;
    cpumask_var_t cpus;
   // uint32_t weight;
    uint32_t credit;
    int credit_balance;
    uint32_t runq_sort;
    unsigned ratelimit_us;
    /* Period of master and tick in milliseconds */
    unsigned tslice_ms, tick_period_us, ticks_per_tslice;
    unsigned credits_per_tslice;

    // eaxen
       unsigned eaxen_tslice_ms, eaxen_tick_period_ms;

};


//eaxen
static void csched_tick(void *_cpu);


//static void csched_acct(void *dummy);


static inline int
__vcpu_on_runq(struct csched_vcpu *svc)
{
    return !list_empty(&svc->runq_elem);
}

static inline struct csched_vcpu *
__runq_elem(struct list_head *elem)
{
    return list_entry(elem, struct csched_vcpu, runq_elem);
}

static inline void
__runq_insert(unsigned int cpu, struct csched_vcpu *svc)
{
    const struct list_head * const runq = RUNQ(cpu);
    struct csched_pcpu *spc = CSCHED_PCPU(cpu);
    struct csched_vcpu * const scurr = CSCHED_VCPU(current);
    int filled_capacity=0;
    struct csched_dom *sdom = NULL;
    struct list_head *iter;
    // showan
    struct list_head *list_pos = NULL;
    //showan
    int pos=1;
    int pos_counter=1;
    BUG_ON( __vcpu_on_runq(svc) );
    BUG_ON( cpu != svc->vcpu->processor );


    sdom = svc->sdom;
    if(sdom != NULL)
    svc->showan_i_o_pri = sdom->showan_dom_ioq;
    svc->showan_i_o_pri_curr = svc->showan_i_o_pri;

    list_for_each( iter, runq )
    {
        const struct csched_vcpu * const iter_svc = __runq_elem(iter);
        sdom = iter_svc->sdom;
        if(sdom != NULL)
        filled_capacity += sdom->eaxen_dom_budget;

        if ( svc->pri > iter_svc->pri )
        {
        	if(list_pos == NULL)
        	{
        		list_pos = iter;
               pos= pos_counter;
        	}
        }

        // showan

        if ( svc->showan_i_o_pri <= iter_svc->showan_i_o_pri_curr && svc->pri == iter_svc->pri )
        {
        	if(list_pos == NULL)
        	{
        	 list_pos = iter;
        	 pos= pos_counter;
        	}

        }
//showan
        pos_counter++;


    }

    if(list_pos == NULL)
    {
            	 list_pos = iter;
            	 pos= pos_counter;
    }

    sdom = scurr->sdom;
    if(sdom != NULL)
	filled_capacity += sdom->eaxen_dom_budget;

                // pcpu_schedule_unlock_irqrestore(lock, flags, cpu);
    spc->showan_cpu_load = filled_capacity;

    if(filled_capacity > spc->eaxen_core_max_filled_current_period )
    {
    	spc->eaxen_core_max_filled_current_period = filled_capacity;
    }

    /* If the vcpu yielded, try to put it behind one lower-priority
     * runnable vcpu if we can.  The next runq_sort will bring it forward
     * within 30ms if the queue too long. */
   /* if ( test_bit(CSCHED_FLAG_VCPU_YIELD, &svc->flags)
         && __runq_elem(iter)->pri > CSCHED_PRI_IDLE )
    {
        iter=iter->next;

       // Some sanity checks
        BUG_ON(iter == runq);
    } */

    if (svc->pri > CSCHED_PRI_IDLE)
    {
    	if(svc->showan_i_o_pri >= 99) // cpu inetensive
    	{        sdom = svc->sdom;
    		set_bit((sdom->dom->domain_id % 15) + 10 , &spc->showan_io_occupied_pri_cur_sr); // showan?
    	}
    	else

    	 set_bit(svc->showan_i_o_pri , &spc->showan_io_occupied_pri_cur_sr);
    }

   // if(svc->showan_i_o_pri < pos &&  svc->pri > CSCHED_PRI_IDLE )
   // {
    // svc->showan_i_o_collision ++;


   // }


    list_add_tail(&svc->runq_elem, list_pos);


/*
if(svc->vcpu->domain->domain_id == 5 || svc->vcpu->domain->domain_id == 6 )
    printk("[%i.%i] pri=%i cpu=%i  asset=%ld  i_o_pri=%i pos=%i \n",
               		   svc->vcpu->domain->domain_id,
					   svc->vcpu->vcpu_id,
					   svc->pri,
					   svc->vcpu->processor,
					   svc->eaxen_asset,
					   svc->showan_i_o_pri,
					   pos

   					   );

*/
}

static inline void
__runq_remove(struct csched_vcpu *svc)
{
    BUG_ON( !__vcpu_on_runq(svc) );
    list_del_init(&svc->runq_elem);
}


/*
 * Translates node-affinity mask into a cpumask, so that we can use it during
 * actual scheduling. That of course will contain all the cpus from all the
 * set nodes in the original node-affinity mask.
 *
 * Note that any serialization needed to access mask safely is complete
 * responsibility of the caller of this function/hook.
 */
static void csched_set_node_affinity(
    const struct scheduler *ops,
    struct domain *d,
    nodemask_t *mask)
{
    struct csched_dom *sdom;
    int node;

    /* Skip idle domain since it doesn't even have a node_affinity_cpumask */
    if ( unlikely(is_idle_domain(d)) )
        return;

    sdom = CSCHED_DOM(d);
    cpumask_clear(sdom->node_affinity_cpumask);
    for_each_node_mask( node, *mask )
        cpumask_or(sdom->node_affinity_cpumask, sdom->node_affinity_cpumask,
                   &node_to_cpumask(node));
}

#define for_each_csched_balance_step(step) \
    for ( (step) = 0; (step) <= CSCHED_BALANCE_CPU_AFFINITY; (step)++ )


/*
 * vcpu-affinity balancing is always necessary and must never be skipped.
 * OTOH, if a domain's node-affinity is said to be automatically computed
 * (or if it just spans all the nodes), we can safely avoid dealing with
 * node-affinity entirely.
 *
 * Node-affinity is also deemed meaningless in case it has empty
 * intersection with mask, to cover the cases where using the node-affinity
 * mask seems legit, but would instead led to trying to schedule the vcpu
 * on _no_ pcpu! Typical use cases are for mask to be equal to the vcpu's
 * vcpu-affinity, or to the && of vcpu-affinity and the set of online cpus
 * in the domain's cpupool.
 */
static inline int __vcpu_has_node_affinity(const struct vcpu *vc,
                                           const cpumask_t *mask)
{
    const struct domain *d = vc->domain;
    const struct csched_dom *sdom = CSCHED_DOM(d);

    if ( d->auto_node_affinity
         || cpumask_full(sdom->node_affinity_cpumask)
         || !cpumask_intersects(sdom->node_affinity_cpumask, mask) )
        return 0;

    return 1;
}

/*
 * Each csched-balance step uses its own cpumask. This function determines
 * which one (given the step) and copies it in mask. For the node-affinity
 * balancing step, the pcpus that are not part of vc's vcpu-affinity are
 * filtered out from the result, to avoid running a vcpu where it would
 * like, but is not allowed to!
 */
static void
csched_balance_cpumask(const struct vcpu *vc, int step, cpumask_t *mask)
{
    if ( step == CSCHED_BALANCE_NODE_AFFINITY )
    {
        cpumask_and(mask, CSCHED_DOM(vc->domain)->node_affinity_cpumask,
                    vc->cpu_affinity);

        if ( unlikely(cpumask_empty(mask)) )
            cpumask_copy(mask, vc->cpu_affinity);
    }
    else /* step == CSCHED_BALANCE_CPU_AFFINITY */
        cpumask_copy(mask, vc->cpu_affinity);
}


/// showan

/*
static cpumask_t
showan_save_cores(struct vcpu *vc)
{

	cpumask_t save_cores_mask;
	   int cpu;
	   struct csched_pcpu *spc = NULL;
	  struct csched_vcpu *svc = CSCHED_VCPU(vc);
	   struct csched_dom * sdom = svc->sdom;
	   cpumask_t *online;
	   cpumask_clear(&save_cores_mask);

	  online = cpupool_scheduler_cpumask(vc->domain->cpupool);





	//------------

	 for_each_cpu(cpu, online)
		{
		 spc = CSCHED_PCPU(cpu);
		 if(spc->eaxen_core_max_filled_last_period + sdom->eaxen_dom_budget <= EAXEN_DEFAULT_CORE_CAPACITY  &&  ! test_bit(svc->showan_i_o_pri , &spc->showan_io_occupied_pri))
			 cpumask_set_cpu(cpu, &save_cores_mask );
		}

     return  save_cores_mask;
}



//-------------------------------------------------------------------------> eaxen region

*/
// showan





static struct csched_vcpu * // dele
showan_update_i_o_pri(int cpu)
{
	const struct list_head *  runq = RUNQ(cpu);
	    struct list_head *iter;
	    struct csched_vcpu * snext= NULL;



	    list_for_each( iter, runq )
	    {
	         struct csched_vcpu * iter_svc = __runq_elem(iter);
	        if (iter_svc ->pri > CSCHED_PRI_IDLE  ) //showan?
	        	iter_svc->showan_i_o_pri_curr = iter_svc->showan_i_o_pri_curr -1;

	    }
	    runq = RUNQ(cpu);
	    snext = __runq_elem(runq->next);
	    if (snext-> showan_i_o_pri_curr > 0)
	    {

	    	list_for_each( iter, runq )
	    		    {
	    		        struct csched_vcpu * iter_svc = __runq_elem(iter);
	    		        if (iter_svc ->pri ==   snext->pri &&  iter_svc -> showan_i_o_pri > 10 ) //showan?
	    		        	return iter_svc;

	    		    }

	    }

	    	return snext;

	}


static int
showan_pcpu_update_load(int  cpu)
{

        const struct list_head * const runq = RUNQ(cpu);
         struct csched_vcpu * const scurr = CSCHED_VCPU(current);
         struct list_head *iter;
         int pcpu_load=0;
         struct csched_dom *sdom = NULL;
         struct csched_pcpu *spc = CSCHED_PCPU(cpu);
         //unsigned long flags;
        // spinlock_t *lock;

        // eaxen? should I disable IRQs becouse if not it is possible that the current  vcpu is changed
        // lock = pcpu_schedule_lock_irqsave(cpu, &flags);
               list_for_each( iter, runq )
                   {
                  const struct csched_vcpu * const iter_svc = __runq_elem(iter);
                  sdom = iter_svc->sdom;
                  if(sdom != NULL)

                	  pcpu_load += sdom->eaxen_dom_budget;
                   }

               sdom = scurr->sdom;
               if(sdom != NULL)
            	   pcpu_load += sdom->eaxen_dom_budget;

            // pcpu_schedule_unlock_irqrestore(lock, flags, cpu);
if(pcpu_load > spc->eaxen_core_max_filled_current_period )
{
	spc->eaxen_core_max_filled_current_period = pcpu_load;
}

         //    printk("cpu:%i---> fc:%i \n", cpu, filled_capacity);

                 return  pcpu_load;
}


static int
showan_pcpu_load(int  cpu, int curr , int last )
{
	 struct csched_pcpu *spc = CSCHED_PCPU(cpu);

	    showan_pcpu_update_load(  cpu);

        if (curr == 1 && last == 1)
        {
        	if(spc->eaxen_core_max_filled_current_period > spc->eaxen_core_max_filled_last_period )
        		return spc->eaxen_core_max_filled_current_period ;
        	else
        		return spc->eaxen_core_max_filled_last_period ;


        }
        else
        	if( last== 1)
        		return spc->eaxen_core_max_filled_last_period ;
        	else
        	//	return spc->eaxen_core_max_filled_current_period ; showan??
        		return spc->showan_cpu_load;


}

static int cpu_is_safe_for_vcpu(int cpu, struct csched_vcpu *svc, int curr , int last, int right_pos) // dele
{

		   struct csched_pcpu *spc = CSCHED_PCPU(cpu);

		   struct vcpu *vc= svc->vcpu;
		   struct csched_dom * sdom = svc->sdom;
if(sdom != NULL )
	svc->showan_i_o_pri = sdom->showan_dom_ioq;


// dele
if (cpu ==  vc->processor) // showan?
	return 1;


if (svc->showan_i_o_pri < 99 )
{

	if(right_pos)
	{

			 if((showan_pcpu_load(cpu, curr, last)+ sdom->eaxen_dom_budget ) <= EAXEN_DEFAULT_CORE_CAPACITY  &&  ! test_bit(svc->showan_i_o_pri , &spc->showan_io_occupied_pri_last_sr))
				return 1;
			 else
				 return 0;
	}
	else
	{
		int i;

		if((showan_pcpu_load(cpu, curr, last)+ sdom->eaxen_dom_budget ) <= EAXEN_DEFAULT_CORE_CAPACITY )
			for( i=1; i <= svc->showan_i_o_pri ; i++)
				if (! test_bit(i ,	&spc->showan_io_occupied_pri_last_sr))
					return 1;
		return 0;


	}
}
else
{
	if((showan_pcpu_load(cpu, curr, last) + sdom->eaxen_dom_budget) <= EAXEN_DEFAULT_CORE_CAPACITY &&  ! test_bit((sdom->dom->domain_id % 15) + 10 , &spc->showan_io_occupied_pri_last_sr))
		return 1;
	else
		return 0;
}


}




static void
eaxen_budgeting(void  *_cpu)
{


    unsigned int cpu = (unsigned long)_cpu;

    struct csched_pcpu *spc = CSCHED_PCPU(cpu);
    struct csched_private *prv = CSCHED_PRIV(per_cpu(scheduler, cpu));

    struct list_head *runq;
       // struct list_head *runq, *elem, *next;
       // struct csched_vcpu *svc_elem;
        spinlock_t *lock;
        unsigned long flags;

        struct csched_vcpu * const scurr = CSCHED_VCPU(current);
        struct list_head *iter;
        struct csched_dom *sdom = NULL;

        spc->eaxen_core_budgeting_last++;
        spc->showan_pcpu_has_cpu_overflow =0;
        spc->showan_pcpu_has_io_overflow =0;
        runq = &spc->runq;








      // is it necessary to save IRQs or not````````````````

      lock = pcpu_schedule_lock_irqsave(cpu, &flags);

      list_for_each( iter, runq )
          {
               struct csched_vcpu *iter_svc = __runq_elem(iter);

          /*     printk("[%i.%i] pri=%i flags=%x cpu=%i  asset=%i  i_o_pri=%i i_o_pri_curr=%i collision=%i \n",
            		   iter_svc->vcpu->domain->domain_id,
					   iter_svc->vcpu->vcpu_id,
					   iter_svc->pri,
					   iter_svc->flags,
					   iter_svc->vcpu->processor,
					   iter_svc->eaxen_asset,
					   iter_svc->showan_i_o_pri,
					   iter_svc->showan_i_o_pri_curr,
					   iter_svc->showan_i_o_collision
					   );
*/
              iter_svc->eaxen_vcpu_budgeting_last=  spc->eaxen_core_budgeting_last;
              sdom = iter_svc-> sdom;
              if (sdom != NULL) {// there are vcpus on the runq that are not belonged to any domain. <--ms
              iter_svc->eaxen_asset = sdom->eaxen_dom_budget * SHOWAN_BUDGET_CONSTANT;
              iter_svc->pri =  CSCHED_PRI_TS_UNDER;  // dele

              iter_svc->showan_i_o_collision = 0;
              iter_svc-> showan_it_should_be_migrated_io_col=0;
              }


          }
  // is it necessary to check if scurr is not idle ?. we dont check now
        sdom=scurr->sdom;
        if (sdom != NULL)
        {
      scurr->eaxen_asset= sdom->eaxen_dom_budget * SHOWAN_BUDGET_CONSTANT;
      scurr->eaxen_vcpu_budgeting_last= spc->eaxen_core_budgeting_last;
      scurr->pri =  CSCHED_PRI_TS_UNDER;
      scurr->showan_i_o_collision = 0;
      scurr-> showan_it_should_be_migrated_io_col=0;

        }



      pcpu_schedule_unlock_irqrestore(lock, flags, cpu);


      spc->showan_io_occupied_pri_last_sr= spc->showan_io_occupied_pri_cur_sr;
      spc->showan_io_occupied_pri_cur_sr= 0;

      spc-> eaxen_core_max_filled_last_period =   spc->eaxen_core_max_filled_current_period;
     // printk("++ cpu=%i  cpu_load_last:%i \n", cpu ,spc->eaxen_core_max_filled_last_period);
	  spc->eaxen_core_max_filled_current_period =   showan_pcpu_update_load(cpu);
	  //printk("cpu= %i , currenmax=%i  last_max=%i", cpu, spc->eaxen_core_max_filled_current_period,       spc-> eaxen_core_max_filled_last_period );

	  if(spc->showan_max_io_collision_last_sr > SHOWAN_MAX_TOLERATED_IO_COLLISION)
	          {struct csched_vcpu *temp_svc;

	          printk(" I_O_overflow for %lu  ioc in cpu %i  /n" ,spc->showan_max_io_collision_last_sr, cpu  );
	          	spc->showan_pcpu_has_io_overflow = 1;
	          	temp_svc = spc->showan_svc_vcpu_with_max_io_col;
	          	if(temp_svc != NULL)
	          		temp_svc->showan_it_should_be_migrated_io_col = 1;


	          }
	  if( spc-> eaxen_core_max_filled_last_period > EAXEN_DEFAULT_CORE_CAPACITY)
	  {

		  printk(" CPU_overflow for %i persenatge in cpu %i  /n" ,spc->eaxen_core_max_filled_last_period, cpu  );
		  spc->showan_pcpu_has_cpu_overflow = 1;
	  }

spc->showan_max_io_collision_last_sr = 0;
spc->showan_svc_vcpu_with_max_io_col = NULL;

      // to measure the temperature of the core to turn off the core if it is sleep and its temp is 0
     // eaxen? should we put this in irqsave
      //printk("spc->max_filled cpu:%d --->  %d \n", cpu ,spc->eaxen_core_max_filled_current_period);

      set_timer(&spc->eaxen_ticker, NOW() + MILLISECS(prv->eaxen_tick_period_ms));
}







/*

static int
_eaxen_cpu_pick(const struct scheduler *ops, struct vcpu *vc, bool_t commit)
{
	 cpumask_t cpus;
	    //cpumask_t idlers;
	    cpumask_t *online;
	//    struct csched_pcpu *spc = NULL;
	    int cpu = vc->processor;
	    int balance_step;

	    // showan




	    bool_t selected= 0;
	    int selected_cpu =vc->processor;
	    struct csched_vcpu *svc = CSCHED_VCPU(vc);
	       struct csched_dom *sdom = NULL;
	       sdom = svc->sdom;
	        if (sdom != NULL)
	        	svc->showan_i_o_pri = sdom->showan_dom_ioq;


	    online = cpupool_scheduler_cpumask(vc->domain->cpupool);
	    cpumask_and(&cpus, vc->cpu_affinity, online);

	    for_each_csched_balance_step( balance_step )
	    {


	        if ( balance_step == CSCHED_BALANCE_NODE_AFFINITY
	             && !__vcpu_has_node_affinity(vc, &cpus) )
	            continue;




	        	csched_balance_cpumask(vc, balance_step, &cpus);
	        		        cpumask_and(&cpus, &cpus, online);


	        		        cpu = cpumask_test_cpu(vc->processor, &cpus)
	        		                ? vc->processor
	        		                : cpumask_cycle(vc->processor, &cpus);



	        		       	        	selected = 0;



	        		        while ( !cpumask_empty(&cpus) )
	        		       	        {




	        		       	            if(selected)
	        		       	            {
	        		       	            if ((showan_pcpu_load(cpu , 1 , 0 ) < showan_pcpu_load(selected_cpu , 1 , 0 )) && cpu_is_safe_for_vcpu(cpu, svc, 1, 0, 1))
	        		       	            {
	        		       	            	selected_cpu= cpu;

	        		       	            }
	        		       	            }
	        		       	            else{
	        		       	            	if(cpu_is_safe_for_vcpu(cpu, svc, 1, 0, 1)){

	        		       	            		selected= 1;
	        		       	            		selected_cpu= cpu;

	        		       	            	}



	        		       	            }




	        		       	            cpumask_clear_cpu(cpu, &cpus);

	        		       	        cpu  = cpumask_cycle(cpu, &cpus);

	        		       	            }
	        		        if(selected)
	        		        	        	return selected_cpu;

csched_balance_cpumask(vc, balance_step, &cpus);
			cpumask_and(&cpus, &cpus, online);


			cpu = cpumask_test_cpu(vc->processor, &cpus)
					? vc->processor
					: cpumask_cycle(vc->processor, &cpus);


			selected = 0;



			while ( !cpumask_empty(&cpus) )
				{




					if(selected)
					{
					if ((showan_pcpu_load(cpu , 1 , 0 ) < showan_pcpu_load(selected_cpu , 1 , 0 )) && cpu_is_safe_for_vcpu(cpu, svc, 1, 0, 0))
					{
						selected_cpu= cpu;

					}
					}
					else{
						if(cpu_is_safe_for_vcpu(cpu, svc, 1, 0, 0)){

							selected= 1;
							selected_cpu= cpu;

						}



					}




					cpumask_clear_cpu(cpu, &cpus);

				cpu  = cpumask_cycle(cpu, &cpus);

					}
		if(selected)
						return selected_cpu;



csched_balance_cpumask(vc, balance_step, &cpus);
							cpumask_and(&cpus, &cpus, online);


							cpu = cpumask_test_cpu(vc->processor, &cpus)
									? vc->processor
									: cpumask_cycle(vc->processor, &cpus);


							selected = 0;



									while ( !cpumask_empty(&cpus) )
										{




											if(selected)
											{
											if (showan_pcpu_load(cpu , 1 , 0 ) < showan_pcpu_load(selected_cpu , 1 , 0 ))
											{
												selected_cpu= cpu;

											}
											}
											else{

													selected= 1;
													selected_cpu= cpu;




											}




											cpumask_clear_cpu(cpu, &cpus);

										cpu  = cpumask_cycle(cpu, &cpus);

											}
								if(selected)
												return selected_cpu;


}

	    return vc->processor;
}

*/


static int
_eaxen_cpu_pick(const struct scheduler *ops, struct vcpu *vc, bool_t commit)
{


	cpumask_t cpus;
	   // cpumask_t idlers;
	    cpumask_t *online;


	//    struct csched_pcpu *spc = NULL;


	    int cpu = vc->processor;
	    int balance_step;



	    // showan





	    bool_t min_found= 0;
	    int selected_cpu =vc->processor;
	    struct csched_vcpu *svc = CSCHED_VCPU(vc);
	       struct csched_dom *sdom = NULL;
	       sdom = svc->sdom;
	        if (sdom != NULL)
	        	svc->showan_i_o_pri = sdom->showan_dom_ioq;


	    online = cpupool_scheduler_cpumask(vc->domain->cpupool);
	    cpumask_and(&cpus, vc->cpu_affinity, online);

	    for_each_csched_balance_step( balance_step )
	    {


	        if ( balance_step == CSCHED_BALANCE_NODE_AFFINITY
	             && !__vcpu_has_node_affinity(vc, &cpus) )
	            continue;






	        	csched_balance_cpumask(vc, balance_step, &cpus);
	        		        cpumask_and(&cpus, &cpus, online);


	        		        cpu = cpumask_test_cpu(vc->processor, &cpus)
	        		                ? vc->processor
	        		                : cpumask_cycle(vc->processor, &cpus);


	        		        if(cpu != vc->processor) // to move the vcpu to a core that does not violates affinity
	        		        	return cpu;


	        		        selected_cpu = vc->processor;
	        		        min_found =0;

	        		        if (cpumask_test_cpu(vc->processor, &cpus))
	        		        	        	  cpumask_clear_cpu(vc->processor, &cpus);


	        		        while ( !cpumask_empty(&cpus) )
	        		       	        {
	        		        	int nxt;


	        		       	            nxt = cpumask_cycle(cpu, &cpus);



	        		       	            if ((showan_pcpu_load(nxt , 1 , 0 ) < showan_pcpu_load(selected_cpu , 1, 0 )) && cpu_is_safe_for_vcpu(nxt, svc, 1, 0, 1))
	        		       	            {
	        		       	            	selected_cpu= nxt;
	        		       	            	min_found = 1;

	        		       	            }



	        		       	            cpu= nxt;

	        		       	            cpumask_clear_cpu(cpu, &cpus);

	        		       	            }
	        		        if(min_found)
	        		        {
	        		        	int avg = (showan_pcpu_load(vc->processor , 1 , 0 ) + showan_pcpu_load(selected_cpu , 1 , 0 )) /2;
	        		        	//  printk("sec 1\n");


	        		        		        	if(sdom->eaxen_dom_budget <= ( showan_pcpu_load(vc->processor , 1 , 0 ) - avg) )

	        		        		        	return selected_cpu;


	        		        }




}



	    return vc->processor;


}






/*
static int
_eaxen_cpu_pick(const struct scheduler *ops, struct vcpu *vc, bool_t commit)
{


	cpumask_t cpus;
	    cpumask_t idlers;
	    cpumask_t *online;


	//    struct csched_pcpu *spc = NULL;


	    int cpu = vc->processor;
	    int balance_step;



	    // showan





	    bool_t min_found= 0;
	    int selected_cpu =vc->processor;
	    struct csched_vcpu *svc = CSCHED_VCPU(vc);
	       struct csched_dom *sdom = NULL;
	       sdom = svc->sdom;
	        if (sdom != NULL)
	        	svc->showan_i_o_pri = sdom->showan_dom_ioq;


	    online = cpupool_scheduler_cpumask(vc->domain->cpupool);
	    cpumask_and(&cpus, vc->cpu_affinity, online);

	    for_each_csched_balance_step( balance_step )
	    {


	        if ( balance_step == CSCHED_BALANCE_NODE_AFFINITY
	             && !__vcpu_has_node_affinity(vc, &cpus) )
	            continue;


	        csched_balance_cpumask(vc, balance_step, &cpus);
	        cpumask_and(&cpus, &cpus, online);


	        cpu = cpumask_test_cpu(vc->processor, &cpus)
	                ? vc->processor
	                : cpumask_cycle(vc->processor, &cpus);
	        ASSERT(cpumask_test_cpu(cpu, &cpus));


	        cpumask_and(&idlers, &cpu_online_map, CSCHED_PRIV(ops)->idlers);
	        if ( vc->processor == cpu && IS_RUNQ_IDLE(cpu) )
	            cpumask_set_cpu(cpu, &idlers);
	        cpumask_and(&cpus, &cpus, &idlers);


	        if ( !cpumask_test_cpu(cpu, &cpus) && !cpumask_empty(&cpus) )
	            cpu = cpumask_cycle(cpu, &cpus);
	       // cpumask_clear_cpu(cpu, &cpus);


	        if (cpumask_test_cpu(vc->processor, &cpus))
	        	  cpumask_clear_cpu(vc->processor, &cpus);



	        	min_found = 0;
	        	cpu= vc->processor;
	        	selected_cpu = vc->processor;


	        while ( !cpumask_empty(&cpus) )
	        {
	        	int nxt;


	            nxt = cpumask_cycle(cpu, &cpus);




	            if ((showan_pcpu_load(nxt , 1 , 0 ) < showan_pcpu_load(selected_cpu , 1 , 0 )) && cpu_is_safe_for_vcpu(nxt, svc, 1, 0, 1))
	            {
	            	selected_cpu= nxt;
	            	min_found = 1;



	            }


	            cpu= nxt;

	            cpumask_clear_cpu(cpu, &cpus);

	            }


	        if(min_found)
	        {
	        	int avg = (showan_pcpu_load(vc->processor , 1 , 0 ) + showan_pcpu_load(selected_cpu , 1 , 0 )) /2;
	        	  printk("sec 0\n");


	        	if(sdom->eaxen_dom_budget <= ( showan_pcpu_load(vc->processor , 0 , 1 ) - avg) )

	        	return selected_cpu;



	        }


	        selected_cpu = vc->processor;


	        	csched_balance_cpumask(vc, balance_step, &cpus);
	        		        cpumask_and(&cpus, &cpus, online);


	        		        cpu = cpumask_test_cpu(vc->processor, &cpus)
	        		                ? vc->processor
	        		                : cpumask_cycle(vc->processor, &cpus);


	        		        selected_cpu = cpu;
	        		        min_found =0;

	        		        if (cpumask_test_cpu(vc->processor, &cpus))
	        		        	        	  cpumask_clear_cpu(vc->processor, &cpus);


	        		        while ( !cpumask_empty(&cpus) )
	        		       	        {
	        		        	int nxt;


	        		       	            nxt = cpumask_cycle(cpu, &cpus);



	        		       	            if ((showan_pcpu_load(nxt , 1 , 0 ) < showan_pcpu_load(selected_cpu , 1, 0 )) && cpu_is_safe_for_vcpu(nxt, svc, 1, 0, 1))
	        		       	            {
	        		       	            	selected_cpu= nxt;
	        		       	            	min_found = 1;

	        		       	            }



	        		       	            cpu= nxt;

	        		       	            cpumask_clear_cpu(cpu, &cpus);

	        		       	            }
	        		        if(min_found)
	        		        {
	        		        	int avg = (showan_pcpu_load(vc->processor , 1 , 0 ) + showan_pcpu_load(selected_cpu , 1 , 0 )) /2;
	        		        	  printk("sec 1\n");


	        		        		        	if(sdom->eaxen_dom_budget <= ( showan_pcpu_load(vc->processor , 1 , 0 ) - avg) )

	        		        		        	return selected_cpu;


	        		        }


	        		        	        	csched_balance_cpumask(vc, balance_step, &cpus);
	        		        	        		        		        cpumask_and(&cpus, &cpus, online);


	        		        	        		        		        cpu = cpumask_test_cpu(vc->processor, &cpus)
	        		        	        		        		                ? vc->processor
	        		        	        		        		                : cpumask_cycle(vc->processor, &cpus);


	        		        	        		        		        selected_cpu = cpu;
	        		        	        		        		        min_found =0;

	        		        	        		        		        if (cpumask_test_cpu(vc->processor, &cpus))
	        		        	        		        		        	        	  cpumask_clear_cpu(vc->processor, &cpus);

	        		        	        		        		        while ( !cpumask_empty(&cpus) )
	        		        	        		        		       	        { int nxt;


	        		        	        		        		       	            nxt = cpumask_cycle(cpu, &cpus);




	        		        	        		        		       	            if ((showan_pcpu_load(nxt , 1 ,0 ) < showan_pcpu_load(selected_cpu , 1 , 0)) && cpu_is_safe_for_vcpu(nxt, svc, 1, 0, 0))
	        		        	        		        		       	            {
	        		        	        		        		       	            	selected_cpu= nxt;
	        		        	        		        		       	            	min_found = 1;


	        		        	        		        		       	            }






	        		        	        		        		       	            cpu= nxt;

	        		        	        		        		       	            cpumask_clear_cpu(cpu, &cpus);


	        		        	        }




	        		        if (min_found)
	        		        {
	        		        	int avg = (showan_pcpu_load(vc->processor , 1 , 0 ) + showan_pcpu_load(selected_cpu , 1 , 0 )) /2;
	        		        	  printk("sec 2\n");


							if(sdom->eaxen_dom_budget <= ( showan_pcpu_load(vc->processor , 1, 0 ) - avg) )


	        		        	return selected_cpu;

	        		        }

	        		        	csched_balance_cpumask(vc, balance_step, &cpus);
	        		        		        		        cpumask_and(&cpus, &cpus, online);


	        		        		        		        cpu = cpumask_test_cpu(vc->processor, &cpus)
	        		        		        		                ? vc->processor
	        		        		        		                : cpumask_cycle(vc->processor, &cpus);

	        		        		        		        selected_cpu = cpu;
															min_found =0;

															if (cpumask_test_cpu(vc->processor, &cpus))
															  cpumask_clear_cpu(vc->processor, &cpus);
	        		        		        		        while ( !cpumask_empty(&cpus) )
	        		        		        		       	        {
	        		        		        		        	int nxt;

	        		        		        		       	            nxt = cpumask_cycle(cpu, &cpus);





	        		        		        		       	            if (showan_pcpu_load(nxt , 1 , 0 ) < showan_pcpu_load(selected_cpu , 1, 0))
	        		        		        		       	            {
	        		        		        		       	            	selected_cpu= nxt;
	        		        		        		       	            	min_found = 1;

	        		        		        		       	            }






	        		        		        		       	            cpu= nxt;

	        		        		        		       	            cpumask_clear_cpu(cpu, &cpus);


	        		        		        		       	        		}
	        		        		        		        	if(min_found)
	        		        		        		        	{
															int avg = (showan_pcpu_load(vc->processor , 1 , 0 ) + showan_pcpu_load(selected_cpu , 1 , 0 )) /2;
															  printk("sec 3\n");


															if(sdom->eaxen_dom_budget <= ( showan_pcpu_load(vc->processor , 1 , 0 ) - avg) )


															return selected_cpu;



	        		        		        		        	}




}

	    printk("sec 4\n");

	    return vc->processor;


}

*/



static bool_t __read_mostly opt_tickle_one_idle = 1;
boolean_param("tickle_one_idle_cpu", opt_tickle_one_idle);

DEFINE_PER_CPU(unsigned int, last_tickle_cpu);




static inline void
__runq_tickle(unsigned int cpu, struct csched_vcpu *new)
{
    struct csched_vcpu * const cur = CSCHED_VCPU(curr_on_cpu(cpu));
    struct csched_private *prv = CSCHED_PRIV(per_cpu(scheduler, cpu));
    cpumask_t mask, idle_mask;
    int balance_step, idlers_empty;

    ASSERT(cur);
    cpumask_clear(&mask);
    idlers_empty = cpumask_empty(prv->idlers);


    /*
     * If the pcpu is idle, or there are no idlers and the new
     * vcpu is a higher priority than the old vcpu, run it here.
     *
     * If there are idle cpus, first try to find one suitable to run
     * new, so we can avoid preempting cur.  If we cannot find a
     * suitable idler on which to run new, run it here, but try to
     * find a suitable idler on which to run cur instead.
     */
    if ( cur->pri == CSCHED_PRI_IDLE
         || (idlers_empty && new->pri > cur->pri) )
    {
        if ( cur->pri != CSCHED_PRI_IDLE )
            SCHED_STAT_CRANK(tickle_idlers_none);
        cpumask_set_cpu(cpu, &mask);
    }
    else if ( !idlers_empty )
    {
        /*
         * Node and vcpu-affinity balancing loop. For vcpus without
         * a useful node-affinity, consider vcpu-affinity only.
         */
        for_each_csched_balance_step( balance_step )
        {
            int new_idlers_empty;

            if ( balance_step == CSCHED_BALANCE_NODE_AFFINITY
                 && !__vcpu_has_node_affinity(new->vcpu,
                                              new->vcpu->cpu_affinity) )
                continue;

            /* Are there idlers suitable for new (for this balance step)? */
            csched_balance_cpumask(new->vcpu, balance_step,
                                   csched_balance_mask);
            cpumask_and(&idle_mask, prv->idlers, csched_balance_mask);
            new_idlers_empty = cpumask_empty(&idle_mask);

            /*
             * Let's not be too harsh! If there aren't idlers suitable
             * for new in its node-affinity mask, make sure we check its
             * vcpu-affinity as well, before taking final decisions.
             */
            if ( new_idlers_empty
                 && balance_step == CSCHED_BALANCE_NODE_AFFINITY )
                continue;

            /*
             * If there are no suitable idlers for new, and it's higher
             * priority than cur, ask the scheduler to migrate cur away.
             * We have to act like this (instead of just waking some of
             * the idlers suitable for cur) because cur is running.
             *
             * If there are suitable idlers for new, no matter priorities,
             * leave cur alone (as it is running and is, likely, cache-hot)
             * and wake some of them (which is waking up and so is, likely,
             * cache cold anyway).
             */
            if ( new_idlers_empty && new->pri > cur->pri )
            {
                SCHED_STAT_CRANK(tickle_idlers_none);
                SCHED_VCPU_STAT_CRANK(cur, kicked_away);
                SCHED_VCPU_STAT_CRANK(cur, migrate_r);
                SCHED_STAT_CRANK(migrate_kicked_away);
                set_bit(_VPF_migrating, &cur->vcpu->pause_flags);
                cpumask_set_cpu(cpu, &mask);
            }
            else if ( !new_idlers_empty )
            {
                /* Which of the idlers suitable for new shall we wake up? */
                SCHED_STAT_CRANK(tickle_idlers_some);
                if ( opt_tickle_one_idle )
                {
                    this_cpu(last_tickle_cpu) =
                        cpumask_cycle(this_cpu(last_tickle_cpu), &idle_mask);
                    cpumask_set_cpu(this_cpu(last_tickle_cpu), &mask);
                }
                else
                    cpumask_or(&mask, &mask, &idle_mask);
            }

            /* Did we find anyone? */
            if ( !cpumask_empty(&mask) )
                break;
        }
    }

    if ( !cpumask_empty(&mask) )
    {
        if ( unlikely(tb_init_done) )
        {
            /* Avoid TRACE_*: saves checking !tb_init_done each step */
            for_each_cpu(cpu, &mask)
                __trace_var(TRC_CSCHED_TICKLE, 0, sizeof(cpu), &cpu);
        }

        /* Send scheduler interrupts to designated CPUs */
        cpumask_raise_softirq(&mask, SCHEDULE_SOFTIRQ);
    }
}







static void
eaxen_burn_budget(struct csched_vcpu *svc, s_time_t now)
{
    s_time_t delta;
    uint64_t val;
    unsigned int credits;


    ASSERT( svc == CSCHED_VCPU(curr_on_cpu(svc->vcpu->processor)) );

    if ( (delta = now - svc->start_time) <= 0 )
        return;

    val = delta * CSCHED_CREDITS_PER_MSEC + svc->residual;
    svc->residual = do_div(val, MILLISECS(1));
    credits = val;
    ASSERT(credits == val);

    atomic_sub(credits, &svc->credit);

    svc->eaxen_asset =  svc->eaxen_asset -  delta ; // some correted computation should be here
    //printk("[%i.%i] pri=%i flags=%x cpu=%i  asset=%i \n",
          //  svc->vcpu->domain->domain_id,
          //  svc->vcpu->vcpu_id,
          //  svc->pri,
           // svc->flags,
           // svc->vcpu->processor,
			//svc->eaxen_asset );
 // printk("(%i.%i) pri=%i usage=%lu \n",
            // svc->vcpu->domain->domain_id,
             // svc->vcpu->vcpu_id,
             // svc->pri,
              // delta);

if(svc->eaxen_asset < 0)
    svc->pri = CSCHED_PRI_TS_OVER;
else
    svc->pri = CSCHED_PRI_TS_UNDER;


    svc->start_time += (credits * MILLISECS(1)) / CSCHED_CREDITS_PER_MSEC;
}



//----------------------------------------> end of eaxen region





static void
csched_free_pdata(const struct scheduler *ops, void *pcpu, int cpu)
{
    struct csched_private *prv = CSCHED_PRIV(ops);
    struct csched_pcpu *spc = pcpu;
    unsigned long flags;

    if ( spc == NULL )
        return;

    spin_lock_irqsave(&prv->lock, flags);

    prv->credit -= prv->credits_per_tslice;
    prv->ncpus--;
    cpumask_clear_cpu(cpu, prv->idlers);
    cpumask_clear_cpu(cpu, prv->cpus);

    if ( (prv->master == cpu) && (prv->ncpus > 0) )
    {
        prv->master = cpumask_first(prv->cpus);
        migrate_timer(&prv->master_ticker, prv->master);
    }
    kill_timer(&spc->ticker);
    if ( prv->ncpus == 0 )
        kill_timer(&prv->master_ticker);

    spin_unlock_irqrestore(&prv->lock, flags);

    free_cpumask_var(spc->balance_mask);
    xfree(spc);
}

static void *
csched_alloc_pdata(const struct scheduler *ops, int cpu)
{
    struct csched_pcpu *spc;
    struct csched_private *prv = CSCHED_PRIV(ops);
    unsigned long flags;

    /* Allocate per-PCPU info */
    spc = xzalloc(struct csched_pcpu);
    if ( spc == NULL )
        return NULL;

    if ( !alloc_cpumask_var(&spc->balance_mask) )
    {
        xfree(spc);
        return NULL;
    }

    spin_lock_irqsave(&prv->lock, flags);

    /* Initialize/update system-wide config */
    prv->credit += prv->credits_per_tslice;
    prv->ncpus++;
    cpumask_set_cpu(cpu, prv->cpus);



    if ( prv->ncpus == 1 )
    {
        prv->master = cpu;

        // eaxen
        //init_timer(&prv->master_ticker, csched_acct, prv, cpu);
        //set_timer(&prv->master_ticker,NOW() + MILLISECS(prv->tslice_ms));
    }

   init_timer(&spc->ticker, csched_tick, (void *)(unsigned long)cpu, cpu);
   set_timer(&spc->ticker, NOW() + MICROSECS(prv->tick_period_us) );

    //eaxen
    init_timer(&spc->eaxen_ticker, eaxen_budgeting, (void *)(unsigned long)cpu, cpu);
            set_timer(&spc->eaxen_ticker, NOW() + MILLISECS(prv->eaxen_tick_period_ms) );


     // eaxen

            spc->eaxen_core_budgeting_last = 0;
            spc->eaxen_core_capacity = EAXEN_DEFAULT_CORE_CAPACITY;

            spc->eaxen_core_max_filled_current_period = 0;
            spc-> eaxen_core_max_filled_last_period = 0;



            spc->showan_io_occupied_pri_cur_sr=0;
            spc->showan_io_occupied_pri_last_sr=0;

            spc->showan_max_io_collision_last_sr =0;
            spc->showan_pcpu_has_cpu_overflow = 0;
            spc->showan_pcpu_has_io_overflow =0;
            spc->showan_svc_vcpu_with_max_io_col= NULL;
            spc->showan_cpu_load =0;


    INIT_LIST_HEAD(&spc->runq);
    spc->runq_sort_last = prv->runq_sort;
    spc->idle_bias = nr_cpu_ids - 1;
    if ( per_cpu(schedule_data, cpu).sched_priv == NULL )
        per_cpu(schedule_data, cpu).sched_priv = spc;

    /* Start off idling... */
    BUG_ON(!is_idle_vcpu(curr_on_cpu(cpu)));
    cpumask_set_cpu(cpu, prv->idlers);

    spin_unlock_irqrestore(&prv->lock, flags);

    return spc;
}

#ifndef NDEBUG
static inline void
__csched_vcpu_check(struct vcpu *vc)
{
    struct csched_vcpu * const svc = CSCHED_VCPU(vc);
    struct csched_dom * const sdom = svc->sdom;

    BUG_ON( svc->vcpu != vc );
    BUG_ON( sdom != CSCHED_DOM(vc->domain) );
    if ( sdom )
    {
        BUG_ON( is_idle_vcpu(vc) );
        BUG_ON( sdom->dom != vc->domain );
    }
    else
    {
        BUG_ON( !is_idle_vcpu(vc) );
    }

    SCHED_STAT_CRANK(vcpu_check);
}
#define CSCHED_VCPU_CHECK(_vc)  (__csched_vcpu_check(_vc))
#else
#define CSCHED_VCPU_CHECK(_vc)
#endif

/*
 * Delay, in microseconds, between migrations of a VCPU between PCPUs.
 * This prevents rapid fluttering of a VCPU between CPUs, and reduces the
 * implicit overheads such as cache-warming. 1ms (1000) has been measured
 * as a good value.
 */
static unsigned int vcpu_migration_delay;
integer_param("vcpu_migration_delay", vcpu_migration_delay);

void set_vcpu_migration_delay(unsigned int delay)
{
    vcpu_migration_delay = delay;
}

unsigned int get_vcpu_migration_delay(void)
{
    return vcpu_migration_delay;
}

static inline int
__csched_vcpu_is_cache_hot(struct vcpu *v)
{
    int hot = ((NOW() - v->last_run_time) <
               ((uint64_t)vcpu_migration_delay * 1000u));

    if ( hot )
        SCHED_STAT_CRANK(vcpu_hot);

    return hot;
}

static inline int
__csched_vcpu_is_migrateable(struct vcpu *vc, int dest_cpu, cpumask_t *mask)
{
    /*
     * Don't pick up work that's in the peer's scheduling tail or hot on
     * peer PCPU. Only pick up work that prefers and/or is allowed to run
     * on our CPU.
     */
    return !vc->is_running &&
           !__csched_vcpu_is_cache_hot(vc) &&
           cpumask_test_cpu(dest_cpu, mask);
}

static int
csched_cpu_pick(const struct scheduler *ops, struct vcpu *vc)
{
    //return _csched_cpu_pick(ops, vc, 1);
	// eaxen
    return  _eaxen_cpu_pick(ops, vc, 1);


}

static inline void
__csched_vcpu_acct_start(struct csched_private *prv, struct csched_vcpu *svc)
{
    struct csched_dom * const sdom = svc->sdom;
    unsigned long flags;

    spin_lock_irqsave(&prv->lock, flags);

    if ( list_empty(&svc->active_vcpu_elem) )
    {
        SCHED_VCPU_STAT_CRANK(svc, state_active);
        SCHED_STAT_CRANK(acct_vcpu_active);

        sdom->active_vcpu_count++;
        list_add(&svc->active_vcpu_elem, &sdom->active_vcpu);

       // prv->weight += sdom->weight; showan
        if ( list_empty(&sdom->active_sdom_elem) )
        {
            list_add(&sdom->active_sdom_elem, &prv->active_sdom);
        }
    }

    TRACE_3D(TRC_CSCHED_ACCOUNT_START, sdom->dom->domain_id,
             svc->vcpu->vcpu_id, sdom->active_vcpu_count);

    spin_unlock_irqrestore(&prv->lock, flags);
}

static inline void
__csched_vcpu_acct_stop_locked(struct csched_private *prv,
    struct csched_vcpu *svc)
{
    struct csched_dom * const sdom = svc->sdom;

    BUG_ON( list_empty(&svc->active_vcpu_elem) );

    SCHED_VCPU_STAT_CRANK(svc, state_idle);
    SCHED_STAT_CRANK(acct_vcpu_idle);

    // BUG_ON( prv->weight < sdom->weight ); showan
    sdom->active_vcpu_count--;
    list_del_init(&svc->active_vcpu_elem);
   // prv->weight -= sdom->weight;          //showan
    if ( list_empty(&sdom->active_vcpu) )
    {
        list_del_init(&sdom->active_sdom_elem);
    }

    TRACE_3D(TRC_CSCHED_ACCOUNT_STOP, sdom->dom->domain_id,
             svc->vcpu->vcpu_id, sdom->active_vcpu_count);
}


static void *
csched_alloc_vdata(const struct scheduler *ops, struct vcpu *vc, void *dd)
{
    struct csched_vcpu *svc;


    /* Allocate per-VCPU info */
    svc = xzalloc(struct csched_vcpu);
    if ( svc == NULL )
        return NULL;

    INIT_LIST_HEAD(&svc->runq_elem);
    INIT_LIST_HEAD(&svc->active_vcpu_elem);
    svc->sdom = dd;
    svc->vcpu = vc;
    svc->pri = is_idle_domain(vc->domain) ?
        CSCHED_PRI_IDLE : CSCHED_PRI_TS_UNDER;
    SCHED_VCPU_STATS_RESET(svc);
    SCHED_STAT_CRANK(vcpu_init);
    // eaxen






     svc->eaxen_vcpu_budgeting_last = 0;
     svc->eaxen_asset = EAXEN_DEFAULT_BUDGET * SHOWAN_BUDGET_CONSTANT; //eaxen?

     //showan
     svc->showan_i_o_collision = 0; // showan?
     svc->showan_i_o_pri = SHOWAN_DEFAULT_IOQ; // showan?
     svc->showan_i_o_pri_curr= SHOWAN_DEFAULT_IOQ;
     svc->showan_it_should_be_migrated_io_col=0;

    return svc;
}

static void
csched_vcpu_insert(const struct scheduler *ops, struct vcpu *vc)
{
    struct csched_vcpu *svc = vc->sched_priv;

    if ( !__vcpu_on_runq(svc) && vcpu_runnable(vc) && !vc->is_running )
        __runq_insert(vc->processor, svc);
}

static void
csched_free_vdata(const struct scheduler *ops, void *priv)
{
    struct csched_vcpu *svc = priv;

    BUG_ON( !list_empty(&svc->runq_elem) );

    xfree(svc);
}

static void
csched_vcpu_remove(const struct scheduler *ops, struct vcpu *vc)
{
    struct csched_private *prv = CSCHED_PRIV(ops);
    struct csched_vcpu * const svc = CSCHED_VCPU(vc);
    struct csched_dom * const sdom = svc->sdom;
    unsigned long flags;

    SCHED_STAT_CRANK(vcpu_destroy);

    if ( test_and_clear_bit(CSCHED_FLAG_VCPU_PARKED, &svc->flags) )
    {
        SCHED_STAT_CRANK(vcpu_unpark);
        vcpu_unpause(svc->vcpu);
    }

    if ( __vcpu_on_runq(svc) )
        __runq_remove(svc);

    spin_lock_irqsave(&(prv->lock), flags);

    if ( !list_empty(&svc->active_vcpu_elem) )
        __csched_vcpu_acct_stop_locked(prv, svc);

    spin_unlock_irqrestore(&(prv->lock), flags);

    BUG_ON( sdom == NULL );
    BUG_ON( !list_empty(&svc->runq_elem) );
}

static void
csched_vcpu_sleep(const struct scheduler *ops, struct vcpu *vc)
{
    struct csched_vcpu * const svc = CSCHED_VCPU(vc);

    SCHED_STAT_CRANK(vcpu_sleep);

    BUG_ON( is_idle_vcpu(vc) );

    if ( curr_on_cpu(vc->processor) == vc )
        cpu_raise_softirq(vc->processor, SCHEDULE_SOFTIRQ);
    else if ( __vcpu_on_runq(svc) )
        __runq_remove(svc);
}



static void
csched_vcpu_wake(const struct scheduler *ops, struct vcpu *vc)
{
    struct csched_vcpu * const svc = CSCHED_VCPU(vc);
    const unsigned int cpu = vc->processor;

    // eaxen
       struct csched_pcpu * const spc = CSCHED_PCPU(cpu);
       struct csched_dom *sdom = NULL;


    BUG_ON( is_idle_vcpu(vc) );

    if ( unlikely(curr_on_cpu(cpu) == vc) )
    {
        SCHED_STAT_CRANK(vcpu_wake_running);
        return;
    }
    if ( unlikely(__vcpu_on_runq(svc)) )
    {
        SCHED_STAT_CRANK(vcpu_wake_onrunq);
        return;
    }

    if ( likely(vcpu_runnable(vc)) )
        SCHED_STAT_CRANK(vcpu_wake_runnable);
    else
        SCHED_STAT_CRANK(vcpu_wake_not_runnable);

    /*
     * We temporarly boost the priority of awaking VCPUs!
     *
     * If this VCPU consumes a non negligeable amount of CPU, it
     * will eventually find itself in the credit accounting code
     * path where its priority will be reset to normal.
     *
     * If on the other hand the VCPU consumes little CPU and is
     * blocking and awoken a lot (doing I/O for example), its
     * priority will remain boosted, optimizing it's wake-to-run
     * latencies.
     *
     * This allows wake-to-run latency sensitive VCPUs to preempt
     * more CPU resource intensive VCPUs without impacting overall
     * system fairness.
     *
     * The one exception is for VCPUs of capped domains unpausing
     * after earning credits they had overspent. We don't boost
     * those.
     */
    if ( svc->pri == CSCHED_PRI_TS_UNDER &&                      // showan ???
         !test_bit(CSCHED_FLAG_VCPU_PARKED, &svc->flags) )
    {
        //svc->pri = CSCHED_PRI_TS_BOOST;  // showan
    	svc->pri = CSCHED_PRI_TS_UNDER;  // showan
    }


    //eaxen

          //  svc->pri = CSCHED_PRI_TS_UNDER;
            if(svc->eaxen_vcpu_budgeting_last != spc->eaxen_core_budgeting_last )
            {

            	sdom= svc->sdom;
            	if (sdom != NULL)
            	{
                svc->eaxen_asset= sdom->eaxen_dom_budget * SHOWAN_BUDGET_CONSTANT;
                svc->pri= CSCHED_PRI_TS_UNDER;
                svc->eaxen_vcpu_budgeting_last = spc->eaxen_core_budgeting_last;
                svc->showan_i_o_collision = 0;
                svc->showan_it_should_be_migrated_io_col = 0;
            	}
            }


        //    if( spc-> eaxen_core_max_filled_last_period > EAXEN_DEFAULT_CORE_CAPACITY)
            	//cpu_overloade= 1;
            //if ( test_bit(svc->showan_i_o_pri , &spc->showan_io_occupied_pri))
            	//io_collision= 1;
            //eaxen



    /* Put the VCPU on the runq and tickle CPUs */
    __runq_insert(cpu, svc);
     __runq_tickle(cpu, svc);
    //__eaxen_runq_tickle(cpu, svc );
}

static void
csched_vcpu_yield(const struct scheduler *ops, struct vcpu *vc)
{
    struct csched_vcpu * const svc = CSCHED_VCPU(vc);

    /* Let the scheduler know that this vcpu is trying to yield */
    set_bit(CSCHED_FLAG_VCPU_YIELD, &svc->flags);
}

static int
csched_dom_cntl(
    const struct scheduler *ops,
    struct domain *d,
    struct xen_domctl_scheduler_op *op)
{
    struct csched_dom * const sdom = CSCHED_DOM(d);
    struct csched_private *prv = CSCHED_PRIV(ops);
    unsigned long flags;

    /* Protect both get and put branches with the pluggable scheduler
     * lock. Runq lock not needed anywhere in here. */
    spin_lock_irqsave(&prv->lock, flags);

    if ( op->cmd == XEN_DOMCTL_SCHEDOP_getinfo )
    {
    	// showan
        //op->u.credit.weight = sdom->weight;
        op->u.credit.weight = sdom->eaxen_dom_budget;
       // op->u.credit.cap = sdom->cap;
        op->u.credit.cap = sdom->showan_dom_ioq;
    }
    else
    {
        ASSERT(op->cmd == XEN_DOMCTL_SCHEDOP_putinfo);

        if ( op->u.credit.weight != 0 )
        {
        	//showan
          //  if ( !list_empty(&sdom->active_sdom_elem) )
           // {
             //   prv->weight -= sdom->weight * sdom->active_vcpu_count;
               // prv->weight += op->u.credit.weight * sdom->active_vcpu_count;
           // }
           // sdom->weight = op->u.credit.weight;
        	sdom->eaxen_dom_budget = op->u.credit.weight; // showan
        }

        if ( op->u.credit.cap != (uint16_t)~0U )
           //  sdom->cap = op->u.credit.cap; showan
        	sdom->showan_dom_ioq = op->u.credit.cap;

    }

    spin_unlock_irqrestore(&prv->lock, flags);

    return 0;
}

static inline void
__csched_set_tslice(struct csched_private *prv, unsigned timeslice)
{
    prv->tslice_ms = timeslice;
    prv->ticks_per_tslice = CSCHED_TICKS_PER_TSLICE;
    if ( prv->tslice_ms < prv->ticks_per_tslice )
        prv->ticks_per_tslice = 1;
    prv->tick_period_us = prv->tslice_ms * 1000 / prv->ticks_per_tslice; // showan?
    prv->credits_per_tslice = CSCHED_CREDITS_PER_MSEC * prv->tslice_ms;
}

static int
csched_sys_cntl(const struct scheduler *ops,
                        struct xen_sysctl_scheduler_op *sc)
{
    int rc = -EINVAL;
    xen_sysctl_credit_schedule_t *params = &sc->u.sched_credit;
    struct csched_private *prv = CSCHED_PRIV(ops);

    switch ( sc->cmd )
    {
    case XEN_SYSCTL_SCHEDOP_putinfo:
        if (params->tslice_ms > XEN_SYSCTL_CSCHED_TSLICE_MAX
            || params->tslice_ms < XEN_SYSCTL_CSCHED_TSLICE_MIN
            || (params->ratelimit_us
                && (params->ratelimit_us > XEN_SYSCTL_SCHED_RATELIMIT_MAX
                    || params->ratelimit_us < XEN_SYSCTL_SCHED_RATELIMIT_MIN))
            || MICROSECS(params->ratelimit_us) > MILLISECS(params->tslice_ms) )
                goto out;
        __csched_set_tslice(prv, params->tslice_ms);
        prv->ratelimit_us = params->ratelimit_us;
        /* FALLTHRU */
    case XEN_SYSCTL_SCHEDOP_getinfo:
        params->tslice_ms = prv->tslice_ms;
        params->ratelimit_us = prv->ratelimit_us;
        rc = 0;
        break;
    }
    out:
    return rc;
}

static void *
csched_alloc_domdata(const struct scheduler *ops, struct domain *dom)
{
    struct csched_dom *sdom;

    sdom = xzalloc(struct csched_dom);
    if ( sdom == NULL )
        return NULL;

    if ( !alloc_cpumask_var(&sdom->node_affinity_cpumask) )
    {
        xfree(sdom);
        return NULL;
    }
    cpumask_setall(sdom->node_affinity_cpumask);

    /* Initialize credit and weight */
    INIT_LIST_HEAD(&sdom->active_vcpu);
    INIT_LIST_HEAD(&sdom->active_sdom_elem);
    sdom->dom = dom;
    // sdom->weight = CSCHED_DEFAULT_WEIGHT; // showan
    // eaxen
    sdom->eaxen_dom_budget = EAXEN_DEFAULT_BUDGET;
    // showan
    sdom->showan_dom_ioq = SHOWAN_DEFAULT_IOQ;

    return (void *)sdom;
}

static int
csched_dom_init(const struct scheduler *ops, struct domain *dom)
{
    struct csched_dom *sdom;

    if ( is_idle_domain(dom) )
        return 0;

    sdom = csched_alloc_domdata(ops, dom);
    if ( sdom == NULL )
        return -ENOMEM;

    dom->sched_priv = sdom;

    return 0;
}

static void
csched_free_domdata(const struct scheduler *ops, void *data)
{
    struct csched_dom *sdom = data;

    free_cpumask_var(sdom->node_affinity_cpumask);
    xfree(data);
}

static void
csched_dom_destroy(const struct scheduler *ops, struct domain *dom)
{
    csched_free_domdata(ops, CSCHED_DOM(dom));
}


static void
csched_vcpu_acct(struct csched_private *prv, unsigned int cpu)
{
    struct csched_vcpu * const svc = CSCHED_VCPU(current);
    const struct scheduler *ops = per_cpu(scheduler, cpu);

    ASSERT( current->processor == cpu );
    ASSERT( svc->sdom != NULL );

    /*
     * If this VCPU's priority was boosted when it last awoke, reset it.
     * If the VCPU is found here, then it's consuming a non-negligeable
     * amount of CPU resources and should no longer be boosted.
     */
   // if ( svc->pri == CSCHED_PRI_TS_BOOST ) // showan?
       // svc->pri = CSCHED_PRI_TS_UNDER;

    /*
     * Update credits
     */
    // showan

  //  if ( !is_idle_vcpu(svc->vcpu) )
      //  burn_credits(svc, NOW());

    /*
     * Put this VCPU and domain back on the active list if it was
     * idling.
     *
     * If it's been active a while, check if we'd be better off
     * migrating it to run elsewhere (see multi-core and multi-thread
     * support in csched_cpu_pick()).
     */
    if ( list_empty(&svc->active_vcpu_elem) )
    {
        __csched_vcpu_acct_start(prv, svc);
    }
    else if ( _eaxen_cpu_pick(ops, current, 0) != cpu ) // showan
    {
        SCHED_VCPU_STAT_CRANK(svc, migrate_r);
        SCHED_STAT_CRANK(migrate_running);
        set_bit(_VPF_migrating, &current->pause_flags);
        cpu_raise_softirq(cpu, SCHEDULE_SOFTIRQ);
    }
}



static void
csched_tick(void *_cpu)
{
    unsigned int cpu = (unsigned long)_cpu;
    struct csched_pcpu *spc = CSCHED_PCPU(cpu);
    struct csched_private *prv = CSCHED_PRIV(per_cpu(scheduler, cpu));

    spc->tick++;

    /*
     * Accounting for running VCPU
     */
    if ( !is_idle_vcpu(current) )
        csched_vcpu_acct(prv, cpu);

    /*
     * Check if runq needs to be sorted
     *
     * Every physical CPU resorts the runq after the accounting master has
     * modified priorities. This is a special O(n) sort and runs at most
     * once per accounting period (currently 30 milliseconds).
     */
   // csched_runq_sort(prv, cpu);

    set_timer(&spc->ticker, NOW() + MICROSECS(prv->tick_period_us) );
}

/*

static void
csched_tick(void *_cpu)
{
    unsigned int cpu = (unsigned long)_cpu;
    struct csched_pcpu *spc = CSCHED_PCPU(cpu);
    struct csched_private *prv = CSCHED_PRIV(per_cpu(scheduler, cpu));
    //struct csched_vcpu * const svc = CSCHED_VCPU(current);
    const struct scheduler *ops = per_cpu(scheduler, cpu);

    spc->tick++;


    //if ( !is_idle_vcpu(current) )
     //   csched_vcpu_acct(prv, cpu);



    // showan?
    if ( !is_idle_vcpu(current) )
    {
    	if ( _eaxen_cpu_pick(ops, current, 0) < cpu )
    	    {

    	        set_bit(_VPF_migrating, &current->pause_flags);
    	        cpu_raise_softirq(cpu, SCHEDULE_SOFTIRQ);
    	    }
    }


    //csched_runq_sort(prv, cpu);

    set_timer(&spc->ticker, NOW() + MICROSECS(prv->tick_period_us) );
}

*/



static struct csched_vcpu *
showan_runq_steal(int peer_cpu, int cpu, int pri, int balance_step)
{
     struct csched_pcpu *  peer_pcpu = CSCHED_PCPU(peer_cpu);
    const struct vcpu * const peer_vcpu = curr_on_cpu(peer_cpu);
    struct csched_vcpu *speer;
    struct list_head *iter;
    struct vcpu *vc;


    if ( peer_pcpu != NULL && !is_idle_vcpu(peer_vcpu) )
    {
        list_for_each( iter, &peer_pcpu->runq )
        {
        	speer = __runq_elem(iter);

            if ( speer->pri <= pri )
                break;


            vc = speer->vcpu;
            BUG_ON( is_idle_vcpu(vc) );

           if(peer_pcpu ->showan_pcpu_has_cpu_overflow) {


            if( !cpu_is_safe_for_vcpu(cpu, speer, 1 , 0, 1))  // showan?
            	continue;
           }
           else // we have I/O overflow
           {
        	   if( !cpu_is_safe_for_vcpu(cpu, speer, 1 , 0, 0) || ! speer->showan_it_should_be_migrated_io_col)  // showan?
        		   continue;
           }


            if ( balance_step == CSCHED_BALANCE_NODE_AFFINITY
                 && !__vcpu_has_node_affinity(vc, vc->cpu_affinity) )
                continue;

            csched_balance_cpumask(vc, balance_step, csched_balance_mask);
            if ( __csched_vcpu_is_migrateable(vc, cpu, csched_balance_mask) )
            {

                TRACE_3D(TRC_CSCHED_STOLEN_VCPU, peer_cpu,
                         vc->domain->domain_id, vc->vcpu_id);
                SCHED_VCPU_STAT_CRANK(speer, migrate_q);
                SCHED_STAT_CRANK(migrate_queued);
                WARN_ON(vc->is_urgent);
                __runq_remove(speer);
                  vc->processor = cpu;
                // showan
                if(peer_pcpu ->showan_pcpu_has_cpu_overflow)
                	peer_pcpu->showan_pcpu_has_cpu_overflow = 0;
                else
                	peer_pcpu->showan_pcpu_has_io_overflow = 0;
                return speer;
            }
        }
    }

    SCHED_STAT_CRANK(steal_peer_idle);
    return NULL;
}


static struct csched_vcpu *
showan_overflow_resolver(struct csched_private *prv, int cpu,
     bool_t *stolen)
{
    struct csched_vcpu *speer;
    cpumask_t workers;
    cpumask_t *online;
    int peer_cpu, peer_node, bstep;
    int node = cpu_to_node(cpu);

    // showan
     struct csched_pcpu *  peer_pcpu = NULL;

   // BUG_ON( cpu != snext->vcpu->processor );
    online = cpupool_scheduler_cpumask(per_cpu(cpupool, cpu));


    if ( unlikely(!cpumask_test_cpu(cpu, online)) )
        goto out;



    for_each_csched_balance_step( bstep )
    {

        peer_node = node;
        do
        {

            cpumask_andnot(&workers, online, prv->idlers);
            cpumask_and(&workers, &workers, &node_to_cpumask(peer_node));
            cpumask_clear_cpu(cpu, &workers);

            peer_cpu = cpumask_first(&workers);
            if ( peer_cpu >= nr_cpu_ids )
                goto next_node;
            do
            {



                spinlock_t *lock = pcpu_schedule_trylock(peer_cpu);

                if ( !lock )
                {
                    SCHED_STAT_CRANK(steal_trylock_failed);
                    peer_cpu = cpumask_cycle(peer_cpu, &workers);
                    continue;
                }

                peer_pcpu = CSCHED_PCPU(peer_cpu);

                if(peer_pcpu->showan_pcpu_has_cpu_overflow == 0 && peer_pcpu->showan_pcpu_has_io_overflow== 0)   //showan
                {
                	pcpu_schedule_unlock(lock, peer_cpu);

                	 peer_cpu = cpumask_cycle(peer_cpu, &workers);
                	                    continue;

                }




              speer = cpumask_test_cpu(peer_cpu, online) ?
                		showan_runq_steal(peer_cpu, cpu, -64 , bstep) : NULL;



                //showan
                pcpu_schedule_unlock(lock, peer_cpu);


                if ( speer != NULL )
                {
                    *stolen = 1;
                    return speer;
                }

                peer_cpu = cpumask_cycle(peer_cpu, &workers);

            } while( peer_cpu != cpumask_first(&workers) );

 next_node:
            peer_node = cycle_node(peer_node, node_online_map);
        } while( peer_node != node );
    }

 out:


    return NULL;
}










/*
 * This function is in the critical path. It is designed to be simple and
 * fast for the common case.
 */
static struct task_slice
csched_schedule(
    const struct scheduler *ops, s_time_t now, bool_t tasklet_work_scheduled)
{
    const int cpu = smp_processor_id();
    struct list_head * const runq = RUNQ(cpu);
    struct csched_vcpu * const scurr = CSCHED_VCPU(current);
    struct csched_private *prv = CSCHED_PRIV(ops);
     struct csched_vcpu *snext1 = NULL ;
    struct csched_vcpu *snext2 = NULL ;
    struct csched_vcpu *snext;
    struct task_slice ret;
    s_time_t runtime, tslice;

    //eaxen
     struct csched_pcpu *spc = CSCHED_PCPU(cpu);
     struct csched_vcpu *showan_stolen_vcpu = NULL;

    SCHED_STAT_CRANK(schedule);
    CSCHED_VCPU_CHECK(current);

    runtime = now - current->runstate.state_entry_time;
    if ( runtime < 0 ) /* Does this ever happen? */
        runtime = 0;

    if ( !is_idle_vcpu(scurr->vcpu) )
    {


    	// eaxen temp
    //	eaxen_burn_budget(scurr, now);
//


        /* Update credits of a non-idle VCPU. */
        //burn_credits(scurr, now);
    	eaxen_burn_budget(scurr, now);
        scurr->start_time -= now;
    }
    else
    {
        /* Re-instate a boosted idle VCPU as normal-idle. */
        scurr->pri = CSCHED_PRI_IDLE;
    }

    /* Choices, choices:
     * - If we have a tasklet, we need to run the idle vcpu no matter what.
     * - If sched rate limiting is in effect, and the current vcpu has
     *   run for less than that amount of time, continue the current one,
     *   but with a shorter timeslice and return it immediately
     * - Otherwise, chose the one with the highest priority (which may
     *   be the one currently running)
     * - If the currently running one is TS_OVER, see if there
     *   is a higher priority one waiting on the runqueue of another
     *   cpu and steal it.
     */

    /* If we have schedule rate limiting enabled, check to see
     * how long we've run for. */
    if ( !tasklet_work_scheduled
         && prv->ratelimit_us
         && vcpu_runnable(current)
         && !is_idle_vcpu(current)
         && runtime < MICROSECS(prv->ratelimit_us) )
    {
        snext = scurr;
        snext->start_time += now;
        perfc_incr(delay_ms);
        tslice = MICROSECS(prv->ratelimit_us);
        ret.migrated = 0;
        goto out;
    }
    tslice = MILLISECS(prv->tslice_ms);

    /*
     * Select next runnable local VCPU (ie top of local runq)
     */
    // showan
          showan_stolen_vcpu =  showan_overflow_resolver(prv, cpu, &ret.migrated);
          if( showan_stolen_vcpu != NULL )
           	//printk("\n hi #############################################\n");
            __runq_insert(cpu ,  showan_stolen_vcpu);





    // // showan
    if (!(IS_RUNQ_IDLE(cpu)))
   snext1 = showan_update_i_o_pri(cpu);
  //  snext1 =  __runq_elem(runq->next);
    ret.migrated = 0;


    if ( vcpu_runnable(current) )
       {
       	//showan
           __runq_insert(cpu, scurr);
       }
       else
           BUG_ON( is_idle_vcpu(current) || list_empty(runq) );

    snext2 = __runq_elem(runq->next);

   if(snext1 != NULL && snext1->pri >= snext2->pri )
    	snext = snext1;
   else
    	snext = snext2;
    //snext = __runq_elem(runq->next);
    /* Tasklet work (which runs in idle VCPU context) overrides all else. */
    if ( tasklet_work_scheduled )
    {
        TRACE_0D(TRC_CSCHED_SCHED_TASKLET);
        snext = CSCHED_VCPU(idle_vcpu[cpu]);
        snext->pri = CSCHED_PRI_TS_BOOST;
    }

    /*
     * Clear YIELD flag before scheduling out
     */
    clear_bit(CSCHED_FLAG_VCPU_YIELD, &scurr->flags);

    /*
     * SMP Load balance:
     *
     * If the next highest priority local runnable VCPU has already eaten
     * through its credits, look on other PCPUs to see if we have more
     * urgent work... If not, csched_load_balance() will return snext, but
     * already removed from the runq.
     */


    // eaxen

    //showan_pcpu_update_load(cpu);
   // if(spc->eaxen_core_max_filled_current_period < EAXEN_DEFAULT_CORE_CAPACITY)// I have a room for additional vcpus
   // {

    //	stolen_vcpu	= csched_load_balance(prv, cpu, snext, &ret.migrated);
    //	if (stolen_vcpu != NULL)
    	//	__runq_insert(cpu, stolen_vcpu);

   // }
   //if ( snext->pri > CSCHED_PRI_TS_OVER  || spc->eaxen_core_max_filled_current_period >= EAXEN_DEFAULT_CORE_CAPACITY  )

   //else
     //   snext = csched_load_balance(prv, cpu, snext, &ret.migrated);






       // if ( snext->pri > CSCHED_PRI_IDLE )
       // {
               __runq_remove(snext);

               if ( snext->pri > CSCHED_PRI_IDLE )

             if (  snext->showan_i_o_pri_curr < 0 )
          {
            	 snext->showan_i_o_collision ++;
            	if(snext->showan_i_o_collision > spc->showan_max_io_collision_last_sr)
            	  {
            	   	spc->showan_max_io_collision_last_sr = snext->showan_i_o_collision ;
            	    spc->showan_svc_vcpu_with_max_io_col = snext;

            	   }
            }

          //}
           //else
           //{

        	 //  snext = __runq_elem(runq->next);
        	 //  if ( snext->pri > CSCHED_PRI_IDLE )
        	 //  __runq_remove(snext);
         //  }
              // snext = csched_load_balance(prv, cpu, snext, &ret.migrated);


    /*
     * Update idlers mask if necessary. When we're idling, other CPUs
     * will tickle us when they get extra work.
     */
    if ( snext->pri == CSCHED_PRI_IDLE )
    {
        if ( !cpumask_test_cpu(cpu, prv->idlers) )
        {
            cpumask_set_cpu(cpu, prv->idlers);

          //  spc->showan_io_occupied_pri= 0; //showan?

        }


    }
    else if ( cpumask_test_cpu(cpu, prv->idlers) )
    {
        cpumask_clear_cpu(cpu, prv->idlers);


    }

    if ( !is_idle_vcpu(snext->vcpu) )
        snext->start_time += now;

out:
    /*
     * Return task to run next...
     */
    ret.time = (is_idle_vcpu(snext->vcpu) ?
                -1 : tslice);
    ret.task = snext->vcpu;

    CSCHED_VCPU_CHECK(ret.task);
    return ret;
}

static void
csched_dump_vcpu(struct csched_vcpu *svc)
{
    struct csched_dom * const sdom = svc->sdom;

    printk("[%i.%i] pri=%i flags=%x cpu=%i",
            svc->vcpu->domain->domain_id,
            svc->vcpu->vcpu_id,
            svc->pri,
            svc->flags,
            svc->vcpu->processor);

    if ( sdom )
    {
       // printk(" credit=%i [w=%u,cap=%u]", atomic_read(&svc->credit), // showan
              //  sdom->weight, sdom->cap);
#ifdef CSCHED_STATS
        printk(" (%d+%u) {a/i=%u/%u m=%u+%u (k=%u)}",
                svc->stats.credit_last,
                svc->stats.credit_incr,
                svc->stats.state_active,
                svc->stats.state_idle,
                svc->stats.migrate_q,
                svc->stats.migrate_r,
                svc->stats.kicked_away);
#endif
    }

    printk("\n");
}

static void
csched_dump_pcpu(const struct scheduler *ops, int cpu)
{
    struct list_head *runq, *iter;
    struct csched_pcpu *spc;
    struct csched_vcpu *svc;
    int loop;
#define cpustr keyhandler_scratch

    spc = CSCHED_PCPU(cpu);
    runq = &spc->runq;

    cpumask_scnprintf(cpustr, sizeof(cpustr), per_cpu(cpu_sibling_mask, cpu));
    printk(" sort=%d, sibling=%s, ", spc->runq_sort_last, cpustr);
    cpumask_scnprintf(cpustr, sizeof(cpustr), per_cpu(cpu_core_mask, cpu));
    printk("core=%s\n", cpustr);

    /* current VCPU */
    svc = CSCHED_VCPU(curr_on_cpu(cpu));
    if ( svc )
    {
        printk("\trun: ");
        csched_dump_vcpu(svc);
    }

    loop = 0;
    list_for_each( iter, runq )
    {
        svc = __runq_elem(iter);
        if ( svc )
        {
            printk("\t%3d: ", ++loop);
            csched_dump_vcpu(svc);
        }
    }
#undef cpustr
}

static void
csched_dump(const struct scheduler *ops)
{
    struct list_head *iter_sdom, *iter_svc;
    struct csched_private *prv = CSCHED_PRIV(ops);
    int loop;
    unsigned long flags;

    spin_lock_irqsave(&(prv->lock), flags);

#define idlers_buf keyhandler_scratch

    printk("info:\n"
           "\tncpus              = %u\n"
           "\tmaster             = %u\n"
           "\tcredit             = %u\n"
           "\tcredit balance     = %d\n"
         //  "\tweight             = %u\n" // showan
           "\trunq_sort          = %u\n"
           "\tdefault-weight     = %d\n"
           "\ttslice             = %dms\n"
           "\tratelimit          = %dus\n"
           "\tcredits per msec   = %d\n"
           "\tticks per tslice   = %d\n"
           "\tmigration delay    = %uus\n",
           prv->ncpus,
           prv->master,
           prv->credit,
           prv->credit_balance,
           //prv->weight, // showan
           prv->runq_sort,
           CSCHED_DEFAULT_WEIGHT,
           prv->tslice_ms,
           prv->ratelimit_us,
           CSCHED_CREDITS_PER_MSEC,
           prv->ticks_per_tslice,
           vcpu_migration_delay);

    cpumask_scnprintf(idlers_buf, sizeof(idlers_buf), prv->idlers);
    printk("idlers: %s\n", idlers_buf);

    printk("active vcpus:\n");
    loop = 0;
    list_for_each( iter_sdom, &prv->active_sdom )
    {
        struct csched_dom *sdom;
        sdom = list_entry(iter_sdom, struct csched_dom, active_sdom_elem);

        list_for_each( iter_svc, &sdom->active_vcpu )
        {
            struct csched_vcpu *svc;
            svc = list_entry(iter_svc, struct csched_vcpu, active_vcpu_elem);

            printk("\t%3d: ", ++loop);
            csched_dump_vcpu(svc);
        }
    }
#undef idlers_buf

    spin_unlock_irqrestore(&(prv->lock), flags);
}

static int
csched_init(struct scheduler *ops)
{
    struct csched_private *prv;

    prv = xzalloc(struct csched_private);
    if ( prv == NULL )
        return -ENOMEM;
   /* if ( !zalloc_cpumask_var(&prv->cpus) ||
         !zalloc_cpumask_var(&prv->idlers) )  */  // credit

    // eaxen
    if ( !zalloc_cpumask_var(&prv->cpus) ||
             !zalloc_cpumask_var(&prv->idlers)  	 )
    {
        free_cpumask_var(prv->cpus);
        xfree(prv);
        return -ENOMEM;
    }

    ops->sched_data = prv;
    spin_lock_init(&prv->lock);
    INIT_LIST_HEAD(&prv->active_sdom);
    prv->master = UINT_MAX;

    if ( sched_credit_tslice_ms > XEN_SYSCTL_CSCHED_TSLICE_MAX
         || sched_credit_tslice_ms < XEN_SYSCTL_CSCHED_TSLICE_MIN )
    {
        printk("WARNING: sched_credit_tslice_ms outside of valid range [%d,%d].\n"
               " Resetting to default %u\n",
               XEN_SYSCTL_CSCHED_TSLICE_MIN,
               XEN_SYSCTL_CSCHED_TSLICE_MAX,
               CSCHED_DEFAULT_TSLICE_MS);
        sched_credit_tslice_ms = CSCHED_DEFAULT_TSLICE_MS;
    }

    __csched_set_tslice(prv, sched_credit_tslice_ms);

    //eaxen
         prv->eaxen_tick_period_ms= EAXEN_BUDGETING_PERIOD_MS;
          prv->eaxen_tslice_ms= EAXEN_DEFAULT_TSLICE_MS;


    if ( MICROSECS(sched_ratelimit_us) > MILLISECS(sched_credit_tslice_ms) )
    {
        printk("WARNING: sched_ratelimit_us >"
               "sched_credit_tslice_ms is undefined\n"
               "Setting ratelimit_us to 1000 * tslice_ms\n");
        prv->ratelimit_us = 1000 * prv->tslice_ms;
    }
    else
        prv->ratelimit_us = sched_ratelimit_us;
    return 0;
}

static void
csched_deinit(const struct scheduler *ops)
{
    struct csched_private *prv;

    prv = CSCHED_PRIV(ops);
    if ( prv != NULL )
    {
        free_cpumask_var(prv->cpus);
        free_cpumask_var(prv->idlers);




        xfree(prv);
    }
}

static void csched_tick_suspend(const struct scheduler *ops, unsigned int cpu)
{
    struct csched_pcpu *spc;

    spc = CSCHED_PCPU(cpu);

    stop_timer(&spc->ticker);
}

static void csched_tick_resume(const struct scheduler *ops, unsigned int cpu)
{
    struct csched_private *prv;
    struct csched_pcpu *spc;
    uint64_t now = NOW();

    spc = CSCHED_PCPU(cpu);

    prv = CSCHED_PRIV(ops);

    set_timer(&spc->ticker, now + MICROSECS(prv->tick_period_us)
            - now % MICROSECS(prv->tick_period_us) );
}

static struct csched_private _csched_priv;

const struct scheduler sched_credit_def = {
    .name           = "SMP Credit Scheduler",
    .opt_name       = "credit",
    .sched_id       = XEN_SCHEDULER_CREDIT,
    .sched_data     = &_csched_priv,

    .init_domain    = csched_dom_init,
    .destroy_domain = csched_dom_destroy,

    .insert_vcpu    = csched_vcpu_insert,
    .remove_vcpu    = csched_vcpu_remove,

    .sleep          = csched_vcpu_sleep,
    .wake           = csched_vcpu_wake,
    .yield          = csched_vcpu_yield,

    .adjust         = csched_dom_cntl,
    .adjust_global  = csched_sys_cntl,

    .set_node_affinity  = csched_set_node_affinity,

    .pick_cpu       = csched_cpu_pick,
    .do_schedule    = csched_schedule,

    .dump_cpu_state = csched_dump_pcpu,
    .dump_settings  = csched_dump,
    .init           = csched_init,
    .deinit         = csched_deinit,
    .alloc_vdata    = csched_alloc_vdata,
    .free_vdata     = csched_free_vdata,
    .alloc_pdata    = csched_alloc_pdata,
    .free_pdata     = csched_free_pdata,
    .alloc_domdata  = csched_alloc_domdata,
    .free_domdata   = csched_free_domdata,

    .tick_suspend   = csched_tick_suspend,
    .tick_resume    = csched_tick_resume,
};

